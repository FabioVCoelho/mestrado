| Advantage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Disadvantage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| - **Massive scalability:** Distributed-memory MPI programs scale to many nodes (thousands of ranks), since each process has independent memory.  <br>- **Independent memory domains:** No hardware cache-coherence between MPI processes (each rank has its own address space), so local memory accesses have zero coherence traffic.                                                                                                                                                                     | - **Communication overhead:** Explicit message passing incurs latency and CPU cost, especially for fine-grained or small-data tasks. Network/NUMA contention limits speed-up.  <br>- **Programming burden:** All data movement, synchronization and buffering must be managed by the programmer. This explicit control leads to complex code, boilerplate (e.g. matching sends/receives) and risk of deadlocks or race conditions.                                                                                                                                                 |
| - **Portability and standardization:** MPI is a broadly supported standard with many implementations (OpenMPI, MPICH, vendor libraries) across MIPS, ARM, x86, etc. Applications can be recompiled on different clusters without algorithm changes.  <br>- **Hardware-optimized implementations:** MPI libraries are tuned for the underlying interconnect and CPU architecture (e.g. InfiniBand RDMA, shared-memory intranode buffers), so they can achieve high raw performance on the target hardware. | - **Setup and resource overhead:** Starting an MPI job requires MPI runtime/library initialization (often via `mpirun`), adding startup latency. Each MPI process uses separate resources (stacks, heaps), so total memory footprint is higher.  <br>- **Limited high-level features:** MPI’s low-level API means advanced tasks (dynamic process management, I/O, heterogeneous devices) need explicit handling or separate libraries. There is no built-in shared-memory view, so MPI code cannot directly exploit global variables or easily share cache data across processes. |
| - **Memory scalability:** Total memory grows proportionally with the number of processes – adding nodes adds address space and RAM capacity. This lets massive problems fit in aggregate memory.  <br>- **Cache and locality control:** Each rank can be bound to a CPU core or NUMA domain.  Local data lives in that process’s cache/memory hierarchy, avoiding coherence bottlenecks.                                                                                                                  | - **No shared-memory:** Lacks implicit data sharing – global data must be explicitly communicated or duplicated.  Legacy algorithms relying on a unified address space must be restructured for message passing.  <br>- **NUMA penalties:** If MPI processes on the same node are not NUMA-aware, communication may cross NUMA boundaries, causing costly remote memory access times.  <br>- **Higher memory usage:** Data often replicated in each rank (e.g. each process holds a copy of input data), increasing overall memory footprint.                                      |
| - **Fine-grained control:** MPI gives exact control over when and how data is exchanged (blocking/nonblocking sends, collective algorithms), enabling optimizations (e.g. overlapping communication/computation).  <br>- **Deterministic parallelism:** Absence of shared variables means no unintended data races; behavior depends only on message ordering.  Hybrid models (MPI+OpenMP) allow combining coarse-grained MPI with threaded fine-grain parallelism.                                       | - **Complex code:** MPI is a low-level, verbose API. The programmer must write explicit `MPI_Send/MPI_Recv` calls (or RMA operations) and handle synchronization (barriers, probes). This manual style increases code size and likelihood of bugs compared to implicit shared-memory models.  <br>- **Debugging and maintenance:** Errors (e.g. mismatched tags or ranks) are hard to detect. MPI does not provide as much runtime checking as higher-level frameworks, making debugging distributed code challenging.                                                             |
| - **Optimized collectives and communication:** MPI implementations often provide high-performance collective routines (broadcast, reduce, all-to-all) that leverage tree algorithms or hardware multicast for large data. Intra-node communication can use shared-memory buffers to reduce latency.                                                                                                                                                                                                       | - **Latency and buffering overhead:** Even optimized, sending a message involves buffer copies and protocol overhead. Small messages incur fixed startup costs (latency dominates), so MPI is inefficient for very fine-grained tasks.  <br>- **CPU wait and synchronization cost:** Blocking operations may spin-wait, consuming CPU cycles, or cause context switches. On a heterogenous system (SIMD/SIMT units), idle ranks waste hardware parallelism while waiting for communication to complete.                                                                            |
| - **Hardware-topology awareness:** MPI supports mapping processes to hardware; programmers can shape communicators to match network topology, binding ranks to specific cores/NUMA nodes for locality. Advanced MPI features (like one-sided RMA) can exploit NIC/GPU capabilities.                                                                                                                                                                                                                       | - **No automatic cache/ALU usage:** MPI itself does not manage CPU caches or vector units. Leveraging SIMD/SIMT (e.g. NEON on ARM, GPU threads) must be done inside each MPI process using other APIs. Poor rank-to-cache affinity can cause cache thrashing.  <br>- **Heterogeneous integration:** MPI offers limited direct support for GPUs or accelerators; using them requires additional libraries (CUDA-aware MPI, OpenCL) or custom data transfers outside of MPI.                                                                                                         |

Sources: Authoritative HPC references and documentation were used to compile these points

Parallel Programming - HPC Wiki
https://hpc-wiki.info/hpc/Parallel_Programming

MPI
https://www.pdc.kth.se/polopoly_fs/1.1352288.1724310794!/MPI-2024.pdf

maleshg.wordpress.com
https://maleshg.wordpress.com/wp-content/uploads/2014/06/parallel_programming_slides.pdf

MPI - Distributed Computing made easy - GeeksforGeeks
https://www.geeksforgeeks.org/operating-systems/mpi-distributed-computing-made-easy/

What is Message Passing Interface (MPI)?
https://www.techtarget.com/searchenterprisedesktop/definition/message-passing-interface-MPI


